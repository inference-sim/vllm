# Fast fork Dockerfile - builds on vLLM's official image
# Only overlays Python changes, reuses all compiled C++/CUDA extensions

ARG VLLM_BASE_VERSION=v0.14.1
FROM vllm/vllm-openai:${VLLM_BASE_VERSION}

# Install OpenTelemetry deps early so it stays cached across code changes
RUN python3 -m pip install --no-cache-dir --prefer-binary \
    opentelemetry-sdk==1.26.0 \
    opentelemetry-api==1.26.0 \
    opentelemetry-exporter-otlp==1.26.0 \
    python-json-logger==2.0.7 \
    opentelemetry-semantic-conventions-ai==0.4.1

# Fail fast during build if deps aren't importable
RUN python3 -c "import opentelemetry.sdk; import opentelemetry.exporter.otlp.proto.http.trace_exporter"

# Find where vLLM is installed in the base image
RUN VLLM_LOCATION=$(python3 -c "import vllm; import os; print(os.path.dirname(vllm.__file__))") && \
    echo "vLLM installed at: $VLLM_LOCATION" && \
    echo "$VLLM_LOCATION" > /tmp/vllm_location.txt

# Copy your fork's Python code
COPY vllm /tmp/vllm-fork/

# Overlay your Python changes on top of the installed vLLM
# This preserves all compiled .so files while updating Python code
RUN VLLM_LOCATION=$(cat /tmp/vllm_location.txt) && \
    cp -r /tmp/vllm-fork/* "$VLLM_LOCATION/" && \
    rm -rf /tmp/vllm-fork && \
    echo "Overlaid fork Python code at $VLLM_LOCATION"

# Copy examples directory (includes kv_events_subscriber.py for KV events sidecar)
# Base image WORKDIR is /vllm-workspace, so examples go there
COPY examples /vllm-workspace/examples

# Keep the same entrypoint as vLLM
ENTRYPOINT ["vllm", "serve"]
